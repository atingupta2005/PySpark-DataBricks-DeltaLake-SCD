{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retailAll = '../data/retail-data/all/'\n",
    "flightData2010 = '../data/flight-data/parquet/2010-summary.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **Spak operates on a per-partition basis when executing code**\n",
    "-  Basically all DF Spark code compiles down to an RDD\n",
    "-  When calling a DF transformation, the underlying logic becomes a set of RDD transformations\n",
    "-  SparkContext is the entry point for low-level API functionality accessed through SparkSession\n",
    "-  Main reason to use RDDs is for fine grained control over physical distribution of data (**custom partitioning of data**)\n",
    "-  RDD performance is best via Scala/Java\n",
    "<br>\n",
    "-  RDDs:\n",
    "    -  Caching:\n",
    "        -  ability to cache or persists an RDD\n",
    "        -  ability to specify a storage level [org.apache.spark.storage.StorageLevel] (combinations of memory only, disk only, and off heap)\n",
    "    -  Checkpointing:\n",
    "        -  saves RDD to risk so future computations on that RDD point to its partitions on disk rather than recomputing the RDD from the original source\n",
    "        -  similar to caching except checkpointing is stored only on disk and not in memory (like cache)\n",
    "        -  when checkpointed RDD is referenced it derives from checkpoint instead of source data, which helps improve performance and optimization\n",
    "        <br>\n",
    "-  Shared Variables:\n",
    "    -  broadcast variables\n",
    "    -  accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD to DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1), Row(id=2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.range(3).rdd\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=0)\n",
      "Row(id=1)\n",
      "Row(id=2)\n"
     ]
    }
   ],
   "source": [
    "for i in spark.range(3).rdd.collect(): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(3).rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Local Collection to RDD Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCollection = \"Training on Python, Spark, Scala, ML\".split(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Training', 'on', 'Python,', 'Spark,', 'Scala,', 'ML']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spark.sparkContext.parallelize(myCollection, 2) # sets number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "on\n",
      "Python,\n",
      "Spark,\n",
      "Scala,\n",
      "ML\n"
     ]
    }
   ],
   "source": [
    "words.setName(\"myWords\") # names app for Spark UI\n",
    "for i in words.collect(): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Data Source Read Example_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformation Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "print(words.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Training', 'on', 'Python,', 'Spark,', 'Scala,', 'ML']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark,', 'Scala,']\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "def startsWithS(individual):\n",
    "  return individual.startswith(\"S\")\n",
    "\n",
    "print(words.filter(lambda word: startsWithS(word)).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "\n",
    "words2 = words.map(\n",
    "    lambda word: \n",
    "        (\n",
    "            word, word[0], word.startswith(\"S\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Training', 'T', False),\n",
       " ('on', 'o', False),\n",
       " ('Python,', 'P', False),\n",
       " ('Spark,', 'S', True),\n",
       " ('Scala,', 'S', True),\n",
       " ('ML', 'M', False)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spark,', 'S', True), ('Scala,', 'S', True)]\n"
     ]
    }
   ],
   "source": [
    "print(words2.filter(\n",
    "    lambda record: record[2]\n",
    ").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Training', 'Python,', 'Spark,', 'Scala,', 'on', 'ML']\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "print(words.sortBy(lambda word: len(word) * -1).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Training', 'Python,', 'Scala,']\n",
      "['on', 'Spark,', 'ML']\n"
     ]
    }
   ],
   "source": [
    "# randomSplit\n",
    "fiftyFiftySplit = words.randomSplit([0.5, 0.5], seed=1)\n",
    "\n",
    "print(fiftyFiftySplit[0].collect())\n",
    "print(fiftyFiftySplit[1].collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Actions Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "print(spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Training', 'on', 'Python,', 'Spark,', 'Scala,', 'ML']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordLengthReducer:  on on\n",
      ">= on ML\n",
      "ML\n"
     ]
    }
   ],
   "source": [
    "def wordLengthReducer(leftWord, rightWord):\n",
    "  print(\"wordLengthReducer: \", leftWord, leftWord)\n",
    "  if len(leftWord) < len(rightWord):\n",
    "    print(\"<\", leftWord, rightWord)\n",
    "    return leftWord\n",
    "  else:\n",
    "    print(\">=\", leftWord, rightWord)\n",
    "    return rightWord\n",
    "\n",
    "print(words.reduce(wordLengthReducer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "print(words.count())\n",
    "\n",
    "# countApprox\n",
    "confidence = 0.95\n",
    "timeoutMilliseconds = 400\n",
    "\n",
    "# Approximate version of count() that returns a potentially incomplete result within a timeout, even if not all tasks have finished.\n",
    "print(words.countApprox(timeoutMilliseconds, confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Training': 1, 'on': 1, 'Python,': 1, 'Spark,': 1, 'Scala,': 1, 'ML': 1})\n"
     ]
    }
   ],
   "source": [
    "# countByValue\n",
    "print(words.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "print(words.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Training', 'on', 'Python,']\n",
      "['ML', 'Python,', 'Scala,']\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "print(words.take(3)) # returns values\n",
    "print(words.takeOrdered(3)) # asc order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark,', 'on']\n"
     ]
    }
   ],
   "source": [
    "withReplacement = True\n",
    "numberToTake = 2\n",
    "randomSeed = 100\n",
    "\n",
    "# The parameter withReplacement controls the Uniqueness of sample result\n",
    "# If we treat a Dataset as a bucket of balls, withReplacement=true means, taking a random ball out of the bucket and place it back into it\n",
    "# That means, the same ball can be picked up again.\n",
    "\n",
    "print(words.takeSample(withReplacement, numberToTake)) # random sample from RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD TXT Save (Uncompressed & Compressed) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"/tmp/words_atin\")\n",
    "\n",
    "words.saveAsTextFile(\"/tmp/words_atin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00001\n",
      "part-00000\n",
      "_SUCCESS\n",
      ".part-00001.crc\n",
      ".part-00000.crc\n",
      "._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir(\"/tmp/words_atin\"): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "on\n",
      "Python,\n",
      "\n",
      "\n",
      "Spark,\n",
      "Scala,\n",
      "ML\n"
     ]
    }
   ],
   "source": [
    "!head /tmp/words_atin/part-00000\n",
    "print(\"\\n\")\n",
    "!head /tmp/words_atin/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#shutil.rmtree(\"/tmp/wordsCompressed_atin\")\n",
    "\n",
    "words.saveAsTextFile(\"/tmp/wordsCompressed_atin\", \\\n",
    "                     compressionCodecClass=\"org.apache.hadoop.io.compress.GzipCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000.gz\n",
      "_SUCCESS\n",
      ".part-00001.gz.crc\n",
      ".part-00000.gz.crc\n",
      "part-00001.gz\n",
      "._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir(\"/tmp/wordsCompressed_atin/\"): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Caching Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myWords ParallelCollectionRDD[20] at readRDDFromFile at PythonRDD.scala:247"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _RDD Checkpointing Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree(\"/tmp/checkpointRDD\")\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/checkpointRDD\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Shared Variables_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Broadcast Variables:\n",
    "    -  saves large value on all worker nodes without re-sending to cluster every time (ex: lookup table as function that fits in memory on each executor)\n",
    "    -  avoids deserialization per task on the worker nodes every time variable is used\n",
    "    -  shared immutable variables that are cached on every machine in cluster instead of serialized with every single task\n",
    "    -  the cost of serializing data for every task can be quite expensive thus broadcast variables are a good alternative   \n",
    "<br>\n",
    "-  Accumulators:\n",
    "    -  adds data together from all tasks into a shared result (ex: error logging counter and debugging)\n",
    "    -  mutable variable that updates value via transformations and sends value to driver node in an efficient manner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = \"Corporate Training for - Spark, Scala, Python\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(my_collection, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Broadcast Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementalData = {\"ATT\":1000, \"Corporate\":200,\n",
    "                    \"Training\":-300, \"days\":100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ATT': 1000, 'Corporate': 200, 'Training': -300, 'days': 100}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppBroadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Training', -300),\n",
       " ('on', 0),\n",
       " ('Python,', 0),\n",
       " ('Spark,', 0),\n",
       " ('Scala,', 0),\n",
       " ('ML', 0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0))).sortBy(lambda wordPair: wordPair[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accumulator Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/flight-data/parquet/2010-summary.parquet\n"
     ]
    }
   ],
   "source": [
    "print(flightData2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = spark.read.parquet(flightData2010)\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of flights to or from China\n",
    "accChina = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def accChinaFunc(flight_row):\n",
    "  destination = flight_row[\"DEST_COUNTRY_NAME\"]\n",
    "  origin = flight_row[\"ORIGIN_COUNTRY_NAME\"]\n",
    "  if destination == \"China\":\n",
    "    accChina.add(flight_row[\"count\"])\n",
    "  if origin == \"China\":\n",
    "    accChina.add(flight_row[\"count\"])\n",
    "\n",
    "# runs foreach row in the input DF (flights) and runs function against each row\n",
    "flights.foreach(lambda flight_row: accChinaFunc(flight_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accChina.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m myCollection \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython Spark PySpark DataBricks Advanced Training\u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(myCollection, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "myCollection = \"Python Spark PySpark DataBricks Advanced Training\"\\\n",
    "  .split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(lambda word: (word.lower(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = words.keyBy(lambda word: word.lower()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.mapValues(lambda word: word.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.flatMapValues(lambda word: word.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.keys().collect()\n",
    "keyword.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "distinctChars = words.flatMap(lambda word: list(word.lower())).distinct()\\\n",
    "  .collect()\n",
    "sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\n",
    "words.map(lambda word: (word.lower()[0], word))\\\n",
    "  .sampleByKey(True, sampleMap, 6).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = words.flatMap(lambda word: word.lower())\n",
    "KVcharacters = chars.map(lambda letter: (letter, 1))\n",
    "def maxFunc(left, right):\n",
    "  return max(left, right)\n",
    "def addFunc(left, right):\n",
    "  return left + right\n",
    "nums = sc.parallelize(range(1,31), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1])))\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.aggregate(0, maxFunc, addFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 3\n",
    "nums.treeAggregate(0, maxFunc, addFunc, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.aggregateByKey(0, addFunc, maxFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valToCombiner(value):\n",
    "  return [value]\n",
    "def mergeValuesFunc(vals, valToAppend):\n",
    "  vals.append(valToAppend)\n",
    "  return vals\n",
    "def mergeCombinerFunc(vals1, vals2):\n",
    "  return vals1 + vals2\n",
    "outputPartitions = 6\n",
    "KVcharacters\\\n",
    "  .combineByKey(\n",
    "    valToCombiner,\n",
    "    mergeValuesFunc,\n",
    "    mergeCombinerFunc,\n",
    "    outputPartitions)\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.foldByKey(0, addFunc).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "distinctChars = words.flatMap(lambda word: word.lower()).distinct()\n",
    "charRDD = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD2 = distinctChars.map(lambda c: (c, random.random()))\n",
    "charRDD.cogroup(charRDD2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyedChars = distinctChars.map(lambda c: (c, random.random()))\n",
    "outputPartitions = 10\n",
    "KVcharacters.join(keyedChars).count()\n",
    "KVcharacters.join(keyedChars, outputPartitions).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numRange = sc.parallelize(range(10), 2)\n",
    "words.zip(numRange).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.coalesce(1).getNumPartitions() # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/data/retail-data/all/\")\n",
    "rdd = df.coalesce(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionFunc(key):\n",
    "  import random\n",
    "  if key == 17850 or key == 12583:\n",
    "    return 0\n",
    "  else:\n",
    "    return random.randint(1,2)\n",
    "\n",
    "keyedRDD = rdd.keyBy(lambda row: row[6])\n",
    "keyedRDD\\\n",
    "  .partitionBy(3, partitionFunc)\\\n",
    "  .map(lambda x: x[0])\\\n",
    "  .glom()\\\n",
    "  .map(lambda x: len(set(x)))\\\n",
    "  .take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
