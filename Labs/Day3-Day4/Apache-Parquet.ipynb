{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9eab98a-5468-4bf5-a392-5ba3b669d1f4",
   "metadata": {},
   "source": [
    "# Apache Parquet Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45703b5d-2dad-43aa-8f7a-33fd570c7066",
   "metadata": {},
   "source": [
    "- Columnar file format\n",
    "- Provides optimizations to speed up queries\n",
    "- Far more efficient file format than CSV or JSON\n",
    "- Provides efficient data compression with enhanced performance to handle complex data in bulk.\n",
    "- Spark SQL provides support for both reading and writing Parquet files\n",
    "- Automatically capture the schema of the original data\n",
    "- Reduces data storage by 75% on average\n",
    "- Spark by default supports Parquet in its library hence we don’t need to add any dependency libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a53ac-5fac-40a4-a326-d758a980f769",
   "metadata": {},
   "source": [
    "## Apache Parquet Advantages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674516a-9300-40bf-9a00-da067b2439b4",
   "metadata": {},
   "source": [
    "- Reduces IO operations.\n",
    "- Fetches specific columns that you need to access.\n",
    "- Consumes less space.\n",
    "- Support type-specific encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cb0b5-d519-47b5-a887-4fcdd204b1b9",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee5cf627-7b72-4a0a-8946-edf028b8b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75395073-4e5e-4a32-89e2-8cbc06fae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "data = ((\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1))\n",
    "\n",
    "columns = (\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be8cf223-0017-49c2-a6bd-681716646d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3c168fd-6a2c-4003-a281-aa53a40cdfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4a69e-7c83-42a9-898e-dff847772287",
   "metadata": {},
   "source": [
    "## Spark Write DataFrame to Parquet file format\n",
    "    - Writing Spark DataFrame to Parquet format preserves the column names and data types\n",
    "    - All columns are automatically converted to be nullable for compatibility reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4018051-79ee-4bca-a1f4-a154eeb11b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1aed516f-dc31-4116-89fd-3d303ae96d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\people.parquet\n",
      "\n",
      "07-05-2022  13:02    <DIR>          .\n",
      "07-05-2022  13:02    <DIR>          ..\n",
      "07-05-2022  13:02                24 .part-00000-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet.crc\n",
      "07-05-2022  12:56                24 .part-00000-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet.crc\n",
      "07-05-2022  13:02                24 .part-00001-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet.crc\n",
      "07-05-2022  12:56                24 .part-00001-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet.crc\n",
      "07-05-2022  13:02                24 .part-00002-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet.crc\n",
      "07-05-2022  12:56                24 .part-00002-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet.crc\n",
      "07-05-2022  13:02                24 .part-00003-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet.crc\n",
      "07-05-2022  12:56                24 .part-00003-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet.crc\n",
      "07-05-2022  13:02                 8 ._SUCCESS.crc\n",
      "07-05-2022  13:02             1,683 part-00000-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet\n",
      "07-05-2022  12:56             1,683 part-00000-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet\n",
      "07-05-2022  13:02             1,690 part-00001-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet\n",
      "07-05-2022  12:56             1,690 part-00001-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet\n",
      "07-05-2022  13:02             1,710 part-00002-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet\n",
      "07-05-2022  12:56             1,710 part-00002-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet\n",
      "07-05-2022  13:02             1,708 part-00003-19819abf-7f9d-4494-92f7-eb86cf0412b3-c000.snappy.parquet\n",
      "07-05-2022  12:56             1,708 part-00003-3c93dfc1-c918-43fb-ab5e-4882ce080f24-c000.snappy.parquet\n",
      "07-05-2022  13:02                 0 _SUCCESS\n",
      "              18 File(s)         13,782 bytes\n",
      "               2 Dir(s)  1,792,603,459,584 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir data/people.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77daa15f-bca7-466f-83c2-a0556786a06d",
   "metadata": {},
   "source": [
    "## Spark Read Parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c484754-4e9c-4ccf-b7b8-811ba1f1c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF = spark.read.parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00477f50-f91d-4376-aac5-1043c9234c32",
   "metadata": {},
   "source": [
    "## Append to existing Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dbc7a67-88e9-4136-a8d7-1d42eb965a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433cebaa-75d6-4a0c-9e0c-5c83e6857373",
   "metadata": {},
   "source": [
    "## Using SQL queries on Parquet\n",
    "- We can also create a temporary view on Parquet files and then use it in Spark SQL statements\n",
    "- This temporary table would be available until the SparkContext present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "960737b7-f2f2-4114-a941-516411cc53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438fda53-f644-4a7d-840a-5a618fd7e3fc",
   "metadata": {},
   "source": [
    "- Above predicate on spark parquet file does the file scan which is performance bottleneck like table scan on a traditional database\n",
    "- We should use partitioning in order to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a8e87-db1f-4b5a-b833-892601c4b9a2",
   "metadata": {},
   "source": [
    "## Spark parquet partition – Improving performance\n",
    "- Partitioning is a feature of many databases and data processing frameworks and it is key to make jobs work at scale\n",
    "- We can do a parquet file partition using spark partitionBy() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ff0d067-acb5-4ea4-bf7d-58dab758476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"gender\",\"salary\").parquet(\"data/people2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c659e-bc58-4859-bff7-a2e3eeeed81b",
   "metadata": {},
   "source": [
    "- Parquet Partition creates a folder hierarchy for each spark partition\n",
    "- We have mentioned the first partition as gender followed by salary hence, it creates a salary folder inside the gender folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7457fe56-2dd7-4a42-8c5a-d1c52ed1fc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\people2.parquet\n",
      "\n",
      "07-05-2022  13:03    <DIR>          .\n",
      "07-05-2022  13:03    <DIR>          ..\n",
      "07-05-2022  13:03                 8 ._SUCCESS.crc\n",
      "07-05-2022  13:03    <DIR>          gender=F\n",
      "07-05-2022  13:03    <DIR>          gender=M\n",
      "07-05-2022  13:03                 0 _SUCCESS\n",
      "               2 File(s)              8 bytes\n",
      "               4 Dir(s)  1,792,603,463,680 bytes free\n",
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\people2.parquet\\gender=F\n",
      "\n",
      "07-05-2022  13:03    <DIR>          .\n",
      "07-05-2022  13:03    <DIR>          ..\n",
      "07-05-2022  13:03    <DIR>          salary=-1\n",
      "07-05-2022  13:03    <DIR>          salary=4000\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  1,792,603,463,680 bytes free\n",
      " Volume in drive D is Windows\n",
      " Volume Serial Number is 949A-70EE\n",
      "\n",
      " Directory of D:\\22-Trngs\\2-Confirmed\\5-PySpark-56-hours-Geet\\GH\\Labs\\Day3-Day4\\people2.parquet\\gender=M\n",
      "\n",
      "07-05-2022  13:03    <DIR>          .\n",
      "07-05-2022  13:03    <DIR>          ..\n",
      "07-05-2022  13:03    <DIR>          salary=3000\n",
      "07-05-2022  13:03    <DIR>          salary=4000\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  1,792,603,463,680 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir people2.parquet\n",
    "! dir \"data/people2.parquet\\gender=F\"\n",
    "! dir \"data/people2.parquet\\gender=M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f597552-07b6-4a1d-96ff-1d6e662d3dd1",
   "metadata": {},
   "source": [
    "## Read from partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ba0f529-a1d2-4338-856c-c500072537e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parqDF = spark.read.parquet(\"data/people2.parquet\")\n",
    "parqDF.createOrReplaceTempView(\"Table2\")\n",
    "df = spark.sql(\"select * from Table2  where gender='M' and salary >= 4000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4a14a69-8a37-48f4-a3c6-76bcca130fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eaaccf-f791-4d07-91fd-4f08a159d551",
   "metadata": {},
   "source": [
    "- The execution of this query is significantly faster than the query without partition\n",
    "- It filters the data first on gender and then applies filters on salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623d04b-5571-49b3-ad14-dd695117d0e8",
   "metadata": {},
   "source": [
    "## Spark Read a specific Parquet partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e109e59-2fb1-4ce7-9555-2246a98009f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the data from the gender partition value “M”.\n",
    "parqDF = spark.read.parquet(\"data/people2.parquet/gender=M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba7481e3-01de-465c-bfb1-66ca6329913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|  Robert |          |Williams|42114|  4000|\n",
      "| Michael |      Rose|        |40288|  4000|\n",
      "|   James |          |   Smith|36636|  3000|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parqDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfa440-6474-44d1-a49b-f07e593373cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
